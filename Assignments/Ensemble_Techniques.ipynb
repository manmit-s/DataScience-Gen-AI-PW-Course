{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "**Answer:**  \n",
        "Ensemble Learning is a technique in machine learning where multiple models (often called \"weak learners\") are combined to produce a stronger overall model. The key idea is that by aggregating the predictions of several models, the ensemble can reduce variance, bias, and improve generalization compared to individual models. Common ensemble methods include Bagging, Boosting, and Stacking.\n",
        "\n",
        "---\n",
        "\n",
        " Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "**Answer:**  \n",
        "- **Bagging (Bootstrap Aggregating):**\n",
        "  - Trains multiple models independently on random subsets of the data.\n",
        "  - Reduces variance and helps prevent overfitting.\n",
        "  - Example: Random Forest.\n",
        "\n",
        "- **Boosting:**\n",
        "  - Trains models sequentially, where each model tries to correct the errors of the previous one.\n",
        "  - Reduces bias and can lead to strong learners.\n",
        "  - Example: AdaBoost, Gradient Boosting.\n",
        "\n",
        "---\n",
        "\n",
        " Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "**Answer:**  \n",
        "Bootstrap sampling is a technique where random samples are drawn from the dataset with replacement. In Bagging methods like Random Forest, each model is trained on a different bootstrap sample. This introduces diversity among models, which helps reduce variance and improves the robustness of the ensemble.\n",
        "\n",
        "---\n",
        "\n",
        " Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "**Answer:**  \n",
        "Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample. In ensemble models like Random Forest, OOB samples are used to evaluate the model without needing a separate validation set. The OOB score is the average accuracy of predictions made on these samples, providing an unbiased estimate of model performance.\n",
        "\n",
        "---\n",
        "\n",
        " Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "**Answer:**  \n",
        "- **Single Decision Tree:**\n",
        "  - Feature importance is based on how much each feature reduces impurity (e.g., Gini or entropy) across splits.\n",
        "  - Can be biased if the tree overfits.\n",
        "\n",
        "- **Random Forest:**\n",
        "  - Aggregates feature importance across many trees.\n",
        "  - More stable and reliable due to averaging.\n",
        "  - Reduces bias and highlights consistently important features.\n",
        "\n",
        "---\n",
        "\n",
        " Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Choose between Bagging or Boosting:**\n",
        "   - Use **Boosting** (e.g., XGBoost) if the dataset is noisy and complex, as it focuses on correcting errors.\n",
        "   - Use **Bagging** (e.g., Random Forest) if overfitting is a concern and the dataset is large.\n",
        "\n",
        "2. **Handle Overfitting:**\n",
        "   - Apply regularization techniques (e.g., max_depth, min_samples_split).\n",
        "   - Use cross-validation to tune hyperparameters.\n",
        "   - Prefer ensemble methods that reduce variance (Bagging) or bias (Boosting) based on the problem.\n",
        "\n",
        "3. **Select Base Models:**\n",
        "   - Decision Trees for interpretability.\n",
        "   - Gradient Boosted Trees for performance.\n",
        "   - Try logistic regression or SVMs if features are well-separated.\n",
        "\n",
        "4. **Evaluate Performance Using Cross-Validation:**\n",
        "   - Use k-fold cross-validation to assess model stability.\n",
        "   - Track metrics like accuracy, precision, recall, F1-score, and AUC.\n",
        "\n",
        "5. **Justify Ensemble Learning in Real-World Context:**\n",
        "   - Ensemble methods reduce the risk of relying on a single model.\n",
        "   - They improve prediction accuracy and robustness.\n",
        "   - In financial contexts, better predictions reduce default risk and improve decision-making.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "b1Iv-_c5Va1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get top 5 important features\n",
        "importances = model.feature_importances_\n",
        "indices = importances.argsort()[::-1][:5]\n",
        "\n",
        "# Print top features\n",
        "print(\"Top 5 Important Features:\")\n",
        "for i in indices:\n",
        "    print(f\"{data.feature_names[i]}: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xiq4-y32Vo9t",
        "outputId": "2a9a05ac-5858-484f-ca7f-52617f3ecbf7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area: 0.1394\n",
            "worst concave points: 0.1322\n",
            "mean concave points: 0.1070\n",
            "worst radius: 0.0828\n",
            "worst perimeter: 0.0808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bag_acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxk_8Ur6WGKS",
        "outputId": "5b3f2ea5-a49f-4467-d956-2f1d6137e53d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.00\n",
            "Bagging Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Define model and grid\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "params = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(model, params, cv=3)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRNLsonVWT6T",
        "outputId": "1abc38d8-3d4f-4fc8-fe0c-671b00090eca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'n_estimators': 50}\n",
            "Best Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(f\"Bagging MSE: {bag_mse:.2f}\")\n",
        "print(f\"Random Forest MSE: {rf_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWPiZD7mWZr6",
        "outputId": "fc83438c-c2be-47b5-e231-5c111a5273bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.28\n",
            "Random Forest MSE: 0.25\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: What is a Decision Tree, and how does it work in classification?\n",
        "\n",
        "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. In classification:\n",
        "\n",
        "- Each internal node represents a decision based on a feature.\n",
        "- Each branch represents the outcome of the decision.\n",
        "- Each leaf node represents a class label.\n",
        "\n",
        "The tree splits data recursively based on feature values to create pure subsets. It uses metrics like **Gini Impurity** or **Entropy** to choose the best splits.\n",
        "\n",
        "---\n",
        "\n",
        "## Q2: Explain Gini Impurity and Entropy. How do they impact splits?\n",
        "\n",
        "- **Gini Impurity** measures the probability of misclassification:\n",
        "\n",
        "\n",
        "\n",
        "- **Entropy** measures the disorder or uncertainty:\n",
        "  \n",
        "\n",
        "\n",
        "Both are used to evaluate how well a feature splits the data. Lower impurity or entropy means better splits.\n",
        "\n",
        "---\n",
        "\n",
        "## Q3: Difference between Pre-Pruning and Post-Pruning\n",
        "\n",
        "| Type         | Description | Pros | Cons |\n",
        "|--------------|-------------|------|------|\n",
        "| Pre-Pruning  | Stops tree growth early using constraints like `max_depth`, `min_samples_split` | Faster, avoids overfitting | Might underfit |\n",
        "| Post-Pruning | Builds full tree first, then prunes back using validation | More accurate, flexible | Computationally expensive |\n",
        "\n",
        "---\n",
        "\n",
        "## Q4: What is Information Gain and why is it important?\n",
        "\n",
        "**Information Gain (IG)** measures the reduction in entropy after a split.\n",
        "\n",
        "\n",
        "It helps select the best feature for splitting by quantifying how much uncertainty is reduced.\n",
        "\n",
        "---\n",
        "\n",
        "## Q5: Real-world applications of Decision Trees\n",
        "\n",
        "- **Applications**: Medical diagnosis, loan approval, fraud detection, customer segmentation.\n",
        "- **Advantages**:\n",
        "  - Easy to interpret\n",
        "  - Handles both numerical and categorical data\n",
        "  - Requires minimal preprocessing\n",
        "- **Limitations**:\n",
        "  - Prone to overfitting\n",
        "  - Sensitive to small data changes\n",
        "  - Less accurate than ensemble methods\n",
        "\n",
        "---\n",
        "\n",
        "## Q10: Healthcare Case Study – Step-by-Step Process\n",
        "\n",
        "1. **Handle Missing Values**: Use imputation (`SimpleImputer`) for numerical and categorical features.\n",
        "2. **Encode Categorical Features**: Use `OneHotEncoder` or `OrdinalEncoder`.\n",
        "3. **Train Model**: Fit `DecisionTreeClassifier` on cleaned data.\n",
        "4. **Tune Hyperparameters**: Use `GridSearchCV` to optimize `max_depth`, `min_samples_split`.\n",
        "5. **Evaluate Performance**: Use metrics like accuracy, precision, recall, and confusion matrix.\n",
        "\n",
        "**Business Value**: Enables early disease detection, personalized treatment, and resource optimization—leading to better patient outcomes and reduced costs.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "L2jG_zjcuiYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y, clf.predict(X)))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6vytAi2u3tu",
        "outputId": "4fb02730-0a7e-43bc-e382-b6b60a0484c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.01333333 0.01333333 0.55072262 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "clf_full.fit(X, y)\n",
        "clf_limited.fit(X, y)\n",
        "\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y, clf_full.predict(X)))\n",
        "print(\"Limited Tree Accuracy:\", accuracy_score(y, clf_limited.predict(X)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3OqWoMevCrQ",
        "outputId": "9d1f893c-30d4-4f01-a94d-fe9816ebf934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Limited Tree Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X, y)\n",
        "\n",
        "preds = reg.predict(X)\n",
        "print(\"MSE:\", mean_squared_error(y, preds))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q98oSvoyPqt",
        "outputId": "6f5d87a3-9895-4e3b-d288-d614ac261e16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 9.570289276518477e-32\n",
            "Feature Importances: [0.52509405 0.05087686 0.05481703 0.02650712 0.03115477 0.13090022\n",
            " 0.09486864 0.08578129]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor # Import DecisionTreeRegressor\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeRegressor(), params, cv=5) # Changed to DecisionTreeRegressor\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_) # Note: This will now be R^2 score for regression"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvTMtOoNyTnX",
        "outputId": "b267b5b9-942a-4a7c-fa30-01a500a631ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Best Accuracy: 0.47357814961836614\n"
          ]
        }
      ]
    }
  ]
}
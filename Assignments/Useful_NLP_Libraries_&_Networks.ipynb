{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Compare and contrast NLTK and spaCy in terms of features, ease of use, and performance.\n",
        "\n",
        "**Answer:**\n",
        "- **NLTK (Natural Language Toolkit):**\n",
        "  - Rich collection of linguistic resources (corpora, lexical databases).\n",
        "  - Great for teaching, research, and prototyping.\n",
        "  - Provides granular control over tokenization, parsing, POS tagging.\n",
        "  - Slower performance compared to modern libraries.\n",
        "  - Steeper learning curve due to verbose APIs.\n",
        "\n",
        "- **spaCy:**\n",
        "  - Industrial-strength NLP library optimized for production.\n",
        "  - Fast, efficient, and memory-friendly.\n",
        "  - Built-in pipelines for tokenization, lemmatization, POS tagging, NER.\n",
        "  - Easy integration with deep learning frameworks (TensorFlow, PyTorch).\n",
        "  - Less focused on linguistic theory, more on practical applications.\n",
        "\n",
        "**Comparison:**  \n",
        "NLTK is academic and resource-rich but slower, while spaCy is modern, fast, and production-ready.\n",
        "\n",
        "---\n",
        "\n",
        "### Q2: What is TextBlob and how does it simplify common NLP tasks like sentiment analysis and translation?\n",
        "\n",
        "**Answer:**\n",
        "- **TextBlob** is a Python library built on top of NLTK and Pattern.\n",
        "- Provides a simple API for common NLP tasks:\n",
        "  - **Sentiment Analysis** → Returns polarity (-1 to 1) and subjectivity (0 to 1).\n",
        "  - **Translation** → Uses Google Translate API for multilingual support.\n",
        "  - **Tokenization, POS tagging, noun phrase extraction** with minimal code.\n",
        "- **Simplification:** Abstracts complex NLP pipelines into easy-to-use functions, making it beginner-friendly.\n",
        "\n",
        "---\n",
        "\n",
        "### Q3: Explain the role of Stanford NLP in academic and industry NLP projects.\n",
        "\n",
        "**Answer:**\n",
        "- **Stanford NLP (CoreNLP):**\n",
        "  - Developed by Stanford University, widely used in academia.\n",
        "  - Provides state-of-the-art models for POS tagging, dependency parsing, sentiment analysis, NER.\n",
        "  - Java-based but has Python wrappers for integration.\n",
        "- **Academic Role:** Benchmark tool for linguistic research and NLP experiments.\n",
        "- **Industry Role:** Used in production systems requiring robust linguistic analysis, especially where accuracy is prioritized over speed.\n",
        "\n",
        "---\n",
        "\n",
        "### Q4: Describe the architecture and functioning of a Recurrent Neural Network (RNN).\n",
        "\n",
        "**Answer:**\n",
        "- **Architecture:**\n",
        "  - Sequential model designed for time-series or text data.\n",
        "  - Each unit takes input at time `t` and hidden state from time `t-1`.\n",
        "  - Hidden state acts as memory, capturing context from previous steps.\n",
        "- **Functioning:**\n",
        "  - Processes sequences one step at a time.\n",
        "  - Updates hidden state using weights and activation functions.\n",
        "  - Outputs predictions based on current input and past context.\n",
        "- **Limitation:** Struggles with long-term dependencies due to vanishing/exploding gradients.\n",
        "\n",
        "---\n",
        "\n",
        "### Q5: What is the key difference between LSTM and GRU networks in NLP applications?\n",
        "\n",
        "**Answer:**\n",
        "- **LSTM (Long Short-Term Memory):**\n",
        "  - Uses three gates: input, forget, output.\n",
        "  - Maintains a cell state for long-term memory.\n",
        "  - More complex, higher computational cost.\n",
        "- **GRU (Gated Recurrent Unit):**\n",
        "  - Uses two gates: reset and update.\n",
        "  - Combines hidden and cell state into one.\n",
        "  - Simpler, faster, fewer parameters.\n",
        "- **Key Difference:**  \n",
        "  LSTM is more powerful for capturing long dependencies, while GRU is computationally efficient and often performs similarly with less complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### Q10: You are working on a chatbot for a mental health platform. Explain how you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford NLP to understand and respond to user input effectively. Detail your architecture, data preprocessing pipeline, and any ethical considerations.\n",
        "\n",
        "**Answer:**\n",
        "- **Architecture:**\n",
        "  - Input text → Preprocessing (spaCy/Stanford NLP) → Embedding (Word2Vec/GloVe/BERT) → LSTM/GRU → Dense layers → Response generation.\n",
        "- **Data Preprocessing Pipeline:**\n",
        "  - Tokenization, lemmatization, stopword removal (spaCy).\n",
        "  - Named Entity Recognition for identifying sensitive terms (e.g., emotions, medical conditions).\n",
        "  - Embedding for semantic representation.\n",
        "- **Model:**\n",
        "  - LSTM/GRU captures sequential context of user queries.\n",
        "  - Output layer classifies intent (e.g., “seeking help”, “expressing emotion”).\n",
        "  - Response generator maps intent to empathetic replies.\n",
        "- **Ethical Considerations:**\n",
        "  - Ensure privacy and confidentiality of user data.\n",
        "  - Avoid giving medical diagnoses; provide supportive, non-clinical responses.\n",
        "  - Include escalation mechanism to connect users with human professionals when needed.\n",
        "  - Bias mitigation in training data to avoid harmful stereotypes.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uGh8qksNX3w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Changed to specific English tagger\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is fascinating and widely used in AI applications.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sn-_-DsYPUu",
        "outputId": "f81c8b8f-db47-4c8b-f676-15f2522593ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', 'and', 'widely', 'used', 'in', 'AI', 'applications', '.']\n",
            "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('and', 'CC'), ('widely', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('AI', 'NNP'), ('applications', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is planning to open a new office in Mumbai by 2026.\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} → {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnr62YLQYgJl",
        "outputId": "2e7ee8a9-7877-4b5d-80bb-2177c0eac80b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple → ORG\n",
            "Mumbai → GPE\n",
            "2026 → DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text\n",
        "text = \"I love learning about NLP, but sometimes debugging models can be frustrating.\"\n",
        "\n",
        "# Create TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Sentiment analysis\n",
        "print(\"Polarity:\", blob.sentiment.polarity)   # Range: -1 (negative) to +1 (positive)\n",
        "print(\"Subjectivity:\", blob.sentiment.subjectivity)  # Range: 0 (objective) to 1 (subjective)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjxYnpKZFIm",
        "outputId": "6f5c81e9-ea2c-4ca3-9741-0f777fe15bff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.04999999999999999\n",
            "Subjectivity: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample dataset\n",
        "texts = [\"I love NLP\", \"NLP is challenging\", \"I dislike debugging\"]\n",
        "labels = [1, 1, 0]  # 1 = positive, 0 = negative\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "y = np.array(labels)\n",
        "\n",
        "# Build RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=32, input_length=5),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=10, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Model training complete. Ready for predictions!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXjAnkM-ZIv4",
        "outputId": "2003c474-b15c-4243-a9a5-346c9ec6e64a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.3333 - loss: 0.7154\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6971\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6667 - loss: 0.6798\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.6634\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6477\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.6327\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6667 - loss: 0.6183\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6667 - loss: 0.6044\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6667 - loss: 0.5908\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.5777\n",
            "Model training complete. Ready for predictions!\n"
          ]
        }
      ]
    }
  ]
}